<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My Cool AI Chatbot</title>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <script src="https://kit.fontawesome.com/a076d05399.js" crossorigin="anonymous"></script>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background: linear-gradient(135deg, #f0f4f8, #d9e2ec);
        }
        .chat-container {
            max-height: 70vh;
            overflow-y: auto;
            scroll-behavior: smooth;
        }
        .message-bubble {
            max-width: 80%;
            border-radius: 20px;
            padding: 10px 15px;
            margin-bottom: 10px;
            word-wrap: break-word;
        }
        .user-message {
            background-color: #6366f1; /* Indigo 500 */
            color: white;
            align-self: flex-end;
            border-bottom-right-radius: 5px;
        }
        .ai-message {
            background-color: #e2e8f0; /* Slate 200 */
            color: #1a202c; /* Gray 900 */
            align-self: flex-start;
            border-bottom-left-radius: 5px;
        }
        .image-preview {
            max-width: 100px;
            max-height: 100px;
            border-radius: 10px;
            margin-top: 5px;
        }
        .loading-dots span {
            animation: blink 1s infinite;
        }
        .loading-dots span:nth-child(2) {
            animation-delay: 0.2s;
        }
        .loading-dots span:nth-child(3) {
            animation-delay: 0.4s;
        }
        @keyframes blink {
            0%, 100% { opacity: 0.2; }
            50% { opacity: 1; }
        }
    </style>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-J4E29XDPE6"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-J4E29XDPE6'); // YOUR GOOGLE ANALYTICS MEASUREMENT ID
    </script>
</head>
<body class="flex items-center justify-center min-h-screen p-4">
    <div class="bg-white p-6 rounded-xl shadow-2xl w-full max-w-xl flex flex-col h-[90vh]">
        <h1 class="text-3xl font-bold text-center text-gray-800 mb-6">My Cool AI Chatbot</h1>

        <!-- Chat History -->
        <div id="chat-history" class="flex-1 overflow-y-auto p-4 bg-gray-50 rounded-lg mb-4 shadow-inner">
            <!-- Messages will be appended here -->
            <div class="flex flex-col items-start message-bubble ai-message">
                <span>Hello! I'm your AI Chatbot. How can I assist you today?</span>
            </div>
        </div>

        <!-- Image Preview -->
        <div id="image-preview-container" class="hidden mb-4 p-2 border border-gray-300 rounded-lg bg-gray-100 flex items-center justify-between">
            <img id="selected-image-preview" src="#" alt="Image Preview" class="image-preview object-cover mr-4">
            <span id="image-name" class="text-gray-700 text-sm truncate mr-4"></span>
            <button id="remove-image-button" class="text-red-500 hover:text-red-700 focus:outline-none">
                <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor">
                    <path fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd" />
                </svg>
            </button>
        </div>

        <!-- Input Area -->
        <div class="flex flex-col sm:flex-row items-stretch gap-3">
            <div class="flex-grow relative">
                <input type="text" id="user-input" placeholder="Type your message..." class="flex-grow p-3 border border-gray-300 rounded-xl focus:outline-none focus:ring-2 focus:ring-indigo-500 w-full pr-12">
                <label for="image-upload" class="absolute right-3 top-1/2 -translate-y-1/2 cursor-pointer text-gray-500 hover:text-indigo-600">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 16l4.586-4.586a2 2 0 012.828 0L16 16m-2-2l1.586-1.586a2 2 0 012.828 0L20 14m-6-6h.01M6 20h12a2 2 0 002-2V6a2 2 0 00-2-2H6a2 2 0 00-2 2v12a2 2 0 002 2z" />
                    </svg>
                </label>
                <input type="file" id="image-upload" accept="image/*" class="hidden">
            </div>

            <button id="send-button" class="bg-indigo-600 text-white p-3 rounded-xl hover:bg-indigo-700 focus:outline-none focus:ring-2 focus:ring-indigo-500 shadow-md">
                Send
            </button>
            <button id="voice-input-button" class="bg-blue-600 text-white p-3 rounded-xl hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500 shadow-md">
                <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11a7 7 0 01-14 0v-1m7 1v1m0-1V4a3 3 0 00-3-3H6a3 3 0 00-3 3v12a3 3 0 003 3h12a3 3 0 003-3v-7a7 7 0 00-7-7z" />
                </svg>
            </button>
        </div>
        <div id="status-message" class="text-center text-sm mt-2 text-gray-600"></div>
    </div>

    <script>
        const userInput = document.getElementById('user-input');
        const sendButton = document.getElementById('send-button');
        const voiceInputButton = document.getElementById('voice-input-button');
        const chatHistory = document.getElementById('chat-history');
        const statusMessage = document.getElementById('status-message');
        const imageUpload = document.getElementById('image-upload');
        const imagePreviewContainer = document.getElementById('image-preview-container');
        const selectedImagePreview = document.getElementById('selected-image-preview');
        const imageName = document.getElementById('image-name');
        const removeImageButton = document.getElementById('remove-image-button');

        let chatHistoryArray = [];
        let selectedImageBase64 = null;
        let isVoiceRecording = false;
        let mediaRecorder;
        let audioChunks = [];

        // Function to scroll chat to bottom
        function scrollToBottom() {
            chatHistory.scrollTop = chatHistory.scrollHeight;
        }

        // Display message function
        function displayMessage(message, sender, image = null) {
            const messageElement = document.createElement('div');
            messageElement.classList.add('message-bubble', 'flex', 'flex-col');

            if (sender === 'user') {
                messageElement.classList.add('user-message', 'self-end');
            } else {
                messageElement.classList.add('ai-message', 'self-start');
            }

            if (image) {
                const imgElement = document.createElement('img');
                imgElement.src = image;
                imgElement.classList.add('image-preview', 'mb-2');
                messageElement.appendChild(imgElement);
            }

            const textElement = document.createElement('span');
            textElement.textContent = message;
            messageElement.appendChild(textElement);

            chatHistory.appendChild(messageElement);
            scrollToBottom();
        }

        // Show loading indicator
        function showLoading() {
            const loadingElement = document.createElement('div');
            loadingElement.id = 'loading-indicator';
            loadingElement.classList.add('message-bubble', 'ai-message', 'self-start', 'loading-dots');
            loadingElement.innerHTML = '<span>.</span><span>.</span><span>.</span>';
            chatHistory.appendChild(loadingElement);
            scrollToBottom();
        }

        // Hide loading indicator
        function hideLoading() {
            const loadingElement = document.getElementById('loading-indicator');
            if (loadingElement) {
                loadingElement.remove();
            }
        }

        // Handle sending message
        async function sendMessage() {
            const prompt = userInput.value.trim();
            if (!prompt && !selectedImageBase64) {
                statusMessage.textContent = 'Please enter a message or select an image.';
                setTimeout(() => statusMessage.textContent = '', 3000);
                return;
            }

            displayMessage(prompt, 'user', selectedImageBase64);
            userInput.value = '';
            selectedImageBase64 = null; // Clear selected image
            imagePreviewContainer.classList.add('hidden'); // Hide image preview
            selectedImagePreview.src = '';
            imageName.textContent = '';

            showLoading();

            try {
                // Construct the payload for the Gemini API
                const payloadContents = [{
                    role: "user",
                    parts: [{ text: prompt }]
                }];

                if (selectedImageBase64) {
                    // Add image data to payload if available
                    payloadContents[0].parts.push({
                        inlineData: {
                            mimeType: "image/png", // Assuming PNG, adjust if needed
                            data: selectedImageBase64.split(',')[1] // Remove "data:image/png;base64," prefix
                        }
                    });
                }

                const payload = {
                    contents: payloadContents
                };

                const apiKey = ""; // Canvas will automatically provide the API key
                const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;

                const response = await fetch(apiUrl, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify(payload)
                });

                const result = await response.json();

                if (result.candidates && result.candidates.length > 0 &&
                    result.candidates[0].content && result.candidates[0].content.parts &&
                    result.candidates[0].content.parts.length > 0) {
                    const aiResponse = result.candidates[0].content.parts[0].text;
                    displayMessage(aiResponse, 'ai');
                } else {
                    displayMessage("Sorry, I couldn't get a response. Please try again.", 'ai');
                    console.error("Gemini API response structure unexpected:", result);
                }

            } catch (error) {
                console.error('Error fetching AI response:', error);
                displayMessage('Oops! Something went wrong. Please try again.', 'ai');
            } finally {
                hideLoading();
            }
        }

        // Event listeners
        sendButton.addEventListener('click', sendMessage);
        userInput.addEventListener('keypress', (e) => {
            if (e.key === 'Enter') {
                sendMessage();
            }
        });

        // Image upload handling
        imageUpload.addEventListener('change', function(event) {
            const file = event.target.files[0];
            if (file) {
                const reader = new FileReader();
                reader.onload = function(e) {
                    selectedImageBase64 = e.target.result;
                    selectedImagePreview.src = selectedImageBase64;
                    imageName.textContent = file.name;
                    imagePreviewContainer.classList.remove('hidden');
                };
                reader.readAsDataURL(file);
            } else {
                selectedImageBase64 = null;
                imagePreviewContainer.classList.add('hidden');
                selectedImagePreview.src = '';
                imageName.textContent = '';
            }
        });

        removeImageButton.addEventListener('click', () => {
            selectedImageBase64 = null;
            imageUpload.value = ''; // Clear the file input
            imagePreviewContainer.classList.add('hidden');
            selectedImagePreview.src = '';
            imageName.textContent = '';
        });

        // Voice input handling
        voiceInputButton.addEventListener('click', async () => {
            if (!isVoiceRecording) {
                // Start recording
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    mediaRecorder = new MediaRecorder(stream);
                    mediaRecorder.ondataavailable = (event) => {
                        audioChunks.push(event.data);
                    };
                    mediaRecorder.onstop = async () => {
                        const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                        audioChunks = []; // Clear chunks for next recording

                        // For now, we'll just log the blob and clear.
                        // Actual speech-to-text integration would go here,
                        // calling an API to convert audioBlob to text.
                        // This example does not include a STT API call.
                        // You would send audioBlob to a speech-to-text API here
                        // and then use the transcribed text as the user input.
                        console.log('Audio recorded:', audioBlob);
                        statusMessage.textContent = 'Voice recording complete. (Speech-to-text integration not yet implemented)';
                        setTimeout(() => statusMessage.textContent = '', 5000);
                    };

                    mediaRecorder.start();
                    isVoiceRecording = true;
                    voiceInputButton.classList.add('bg-red-500', 'hover:bg-red-600');
                    voiceInputButton.classList.remove('bg-blue-600', 'hover:bg-blue-700');
                    statusMessage.textContent = 'Recording voice... Click again to stop.';
                } catch (err) {
                    console.error('Error accessing microphone:', err);
                    statusMessage.textContent = 'Microphone access denied or error: ' + err.message;
                    setTimeout(() => statusMessage.textContent = '', 5000);
                }
            } else {
                // Stop recording
                mediaRecorder.stop();
                isVoiceRecording = false;
                voiceInputButton.classList.add('bg-blue-600', 'hover:bg-blue-700');
                voiceInputButton.classList.remove('bg-red-500', 'hover:bg-red-600');
                statusMessage.textContent = 'Processing voice...';
            }
        });

        // Initial scroll to bottom
        scrollToBottom();
    </script>
</body>
</html>

        
            
            
